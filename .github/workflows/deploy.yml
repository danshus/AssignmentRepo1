name: Deploy Sentinel Infrastructure

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: false
        default: 'development'
        type: choice
        options:
          - development
          - staging
          - production
      skip_tests:
        description: 'Skip validation tests'
        required: false
        default: false
        type: boolean
      deploy_apps:
        description: 'Deploy applications after infrastructure'
        required: false
        default: true
        type: boolean

env:
  AWS_REGION: us-west-1
  TF_VERSION: "1.12.2"
  KUBECTL_VERSION: "1.32.0"  # Aligned with EKS cluster version
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID || '721500739616' }}  # Default to known account ID

permissions:
  contents: read
  security-events: write
  actions: read

jobs:
  validate-terraform:
    name: Validate Terraform
    runs-on: ubuntu-latest
    if: github.event.inputs.skip_tests != 'true'
    permissions:
      contents: read
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TF_VERSION }}
        
    - name: Terraform Format Check
      run: terraform fmt -check -recursive
      working-directory: ./terraform
      
    - name: Terraform Init
      run: terraform init
      working-directory: ./terraform
      
    - name: Terraform Validate
      run: terraform validate
      working-directory: ./terraform
      
    - name: Run TFLint
      uses: terraform-linters/setup-tflint@v4
      with:
        tflint_version: v0.47.0
        github_token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: TFLint
      run: |
        # Try with AWS plugin first
        if ! tflint --init; then
          echo "Failed to initialize with AWS plugin, using fallback config..."
          tflint --config=.tflint-no-aws.hcl --init
          tflint --config=.tflint-no-aws.hcl
        else
          tflint
        fi
      working-directory: ./terraform
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: validate-terraform
    if: github.event.inputs.skip_tests != 'true'
    permissions:
      contents: read
      security-events: write
      actions: read
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
        
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v3
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

  deploy-infrastructure:
    name: Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: [validate-terraform, security-scan]
    if: |
      always() && 
      ((github.ref == 'refs/heads/main') || (github.event_name == 'workflow_dispatch')) &&
      (github.event.inputs.skip_tests == 'true' || 
       (needs.validate-terraform.result == 'success' && needs.security-scan.result == 'success'))
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TF_VERSION }}
        
    - name: Terraform Init
      run: terraform init
      working-directory: ./terraform
      
    - name: Terraform Plan
      run: terraform plan -out=tfplan
      working-directory: ./terraform
      
    - name: Terraform Apply
      run: terraform apply tfplan
      working-directory: ./terraform

  validate-kubernetes:
    name: Validate Kubernetes Manifests
    runs-on: ubuntu-latest
    needs: deploy-infrastructure
    if: ((github.ref == 'refs/heads/main') || (github.event_name == 'workflow_dispatch')) && (github.event.inputs.deploy_apps != 'false')
    continue-on-error: true  # Allow this job to fail without stopping the workflow
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup kubectl
      run: |
        curl -LO "https://dl.k8s.io/release/v${{ env.KUBECTL_VERSION }}/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/
        kubectl version --client
        
    - name: Setup kubeval
      run: |
        wget https://github.com/instrumenta/kubeval/releases/latest/download/kubeval-linux-amd64.tar.gz
        tar xf kubeval-linux-amd64.tar.gz
        sudo cp kubeval /usr/local/bin
        
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: Update kubeconfig for Gateway cluster
      run: aws eks update-kubeconfig --name eks-gateway --region ${{ env.AWS_REGION }}
      
    - name: Update kubeconfig for Backend cluster
      run: aws eks update-kubeconfig --name eks-backend --region ${{ env.AWS_REGION }}
      
    - name: Validate Gateway manifests with kubeval
      run: |
        kubeval kubernetes/gateway/*.yaml
        
    - name: Validate Backend manifests with kubeval
      run: |
        kubeval kubernetes/backend/*.yaml
        
    - name: Validate Gateway manifests with kubectl
      run: |
        kubectl apply --dry-run=client -f kubernetes/gateway/ --validate=false || echo "⚠️ Gateway validation failed, but continuing..."
        
    - name: Validate Backend manifests with kubectl
      run: |
        kubectl apply --dry-run=client -f kubernetes/backend/ --validate=false || echo "⚠️ Backend validation failed, but continuing..."

  deploy-applications:
    name: Deploy Applications
    runs-on: ubuntu-latest
    needs: [deploy-infrastructure, validate-kubernetes]
    if: |
      always() && 
      ((github.ref == 'refs/heads/main') || (github.event_name == 'workflow_dispatch')) && 
      (github.event.inputs.deploy_apps != 'false') &&
      (needs.deploy-infrastructure.result == 'success')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup kubectl
      run: |
        curl -LO "https://dl.k8s.io/release/v${{ env.KUBECTL_VERSION }}/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/
        kubectl version --client
        
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: Update kubeconfig for Gateway cluster
      run: aws eks update-kubeconfig --name eks-gateway --region ${{ env.AWS_REGION }}
      
    - name: Update kubeconfig for Backend cluster
      run: aws eks update-kubeconfig --name eks-backend --region ${{ env.AWS_REGION }}
      
    - name: Deploy Backend service
      run: kubectl apply -f kubernetes/backend/
      
    - name: Deploy Gateway proxy
      run: kubectl apply -f kubernetes/gateway/
      
    - name: Deploy AWS Load Balancer Controller
      run: |
        # Install AWS Load Balancer Controller
        kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master"
        
        # Create service account with IRSA
        kubectl create serviceaccount aws-load-balancer-controller -n kube-system --dry-run=client -o yaml | kubectl apply -f -
        kubectl annotate serviceaccount aws-load-balancer-controller -n kube-system \
          eks.amazonaws.com/role-arn=arn:aws:iam::${{ env.AWS_ACCOUNT_ID }}:role/AmazonEKSLoadBalancerControllerRole \
          --overwrite
        
        # Install the controller
        curl -o aws-load-balancer-controller.yaml https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.4.4/v2_4_4_full.yaml
        sed -i 's/your-cluster-name/eks-gateway/g' aws-load-balancer-controller.yaml
        kubectl apply -f aws-load-balancer-controller.yaml
        
        # Wait for controller to be ready
        kubectl rollout status deployment/aws-load-balancer-controller -n kube-system --timeout=300s
      
    - name: Wait for Fargate pods to be scheduled
      run: |
        echo "Waiting for Fargate pods to be scheduled..."
        sleep 30
        
    - name: Wait for Backend deployment
      run: |
        echo "Waiting for Backend deployment to be ready..."
        kubectl rollout status deployment/backend-service -n backend --timeout=600s
        
    - name: Wait for Gateway deployment
      run: |
        echo "Waiting for Gateway deployment to be ready..."
        kubectl rollout status deployment/proxy-service -n gateway --timeout=600s
        
    - name: Wait for ALB to be created
      run: |
        echo "Waiting for ALB to be created by AWS Load Balancer Controller..."
        for i in {1..30}; do
          ALB_URL=$(kubectl get ingress proxy-ingress -n gateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null)
          if [ -n "$ALB_URL" ]; then
            echo "ALB is ready: $ALB_URL"
            break
          fi
          echo "Waiting for ALB... ($i/30)"
          sleep 30
        done
        
    - name: Verify Fargate scheduling
      run: |
        echo "Verifying pods are running on Fargate nodes..."
        kubectl get pods -n gateway -o wide
        kubectl get pods -n backend -o wide
        kubectl get pods -n kube-system -o wide
        
    - name: Test connectivity
      run: |
        # Get the ALB URL from Ingress resource
        ALB_URL=$(kubectl get ingress proxy-ingress -n gateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        echo "ALB URL: $ALB_URL"
        
        if [ -z "$ALB_URL" ]; then
          echo "❌ ALB URL not found in Ingress resource"
          exit 1
        fi
        
        # Wait for ALB to be ready and targets to be healthy
        echo "Waiting for ALB to be ready..."
        sleep 120
        
        # Test the connection with retry logic
        echo "Testing connectivity..."
        for i in {1..5}; do
          if curl -f -m 30 http://$ALB_URL/; then
            echo "✅ Connectivity test passed!"
            break
          else
            echo "❌ Attempt $i failed, retrying in 30 seconds..."
            sleep 30
          fi
        done

  notify:
    name: Notify Deployment Status
    runs-on: ubuntu-latest
    needs: [deploy-applications]
    if: always() && github.ref == 'refs/heads/main'
    
    steps:
    - name: Notify success
      if: needs.deploy-applications.result == 'success'
      run: |
        echo "✅ Deployment successful!"
        echo "Infrastructure deployed successfully to AWS with Fargate"
        
    - name: Notify failure
      if: needs.deploy-applications.result == 'failure'
      run: |
        echo "❌ Deployment failed!"
        echo "Please check the logs for more details" 